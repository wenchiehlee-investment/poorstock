#!/usr/bin/env python3
"""
PoorStock.py - Simple Stock Scraper
Usage: python poorstock.py 2412
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import os
from datetime import datetime
import sys
from pathlib import Path
import re

def format_current_price_table(data, historical_data=None):
    """Format current price data into a proper markdown table."""
    table = []
    table.append("| é …ç›® | åƒ¹æ ¼ |")
    table.append("|------|------|")
    
    headers = ['é–‹ç›¤', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›¤']
    values = []
    
    # Extract numeric values from data
    if data:
        for item in data:
            item_str = str(item).strip()
            if item_str and (item_str.replace('.', '').replace(',', '').isdigit() or 
                           ('.' in item_str and item_str.replace('.', '').replace(',', '').isdigit())):
                values.append(item_str)
    
    # If we don't have enough current price values, try to get from historical data
    if len(values) < 4 and historical_data and len(historical_data) >= 5:
        print(f"ğŸ“Š Current price values found: {len(values)}, using historical data fallback")
        # Find the most recent complete data row (should start with a date)
        for i, item in enumerate(historical_data):
            if re.match(r'202[45]/\d{2}/\d{2}', str(item)):
                # Found a date, next 4 items should be OHLC
                recent_values = []
                for j in range(1, 5):  # Skip date, get next 4 values
                    if i + j < len(historical_data):
                        val = str(historical_data[i + j]).strip()
                        if val and (val.replace('.', '').replace(',', '').isdigit() or 
                                  ('.' in val and val.replace('.', '').replace(',', '').isdigit())):
                            recent_values.append(val)
                
                # Use the most recent historical data if we have 4 values
                if len(recent_values) >= 4:
                    values = recent_values[:4]  # Take first 4 (OHLC)
                    print(f"ğŸ“Š Using recent historical values: {values}")
                    break
    
    # Create table rows
    for i, header in enumerate(headers):
        if i < len(values) and values[i]:
            table.append(f"| {header} | {values[i]} |")
        else:
            table.append(f"| {header} | - |")
    
    table.append("")  # Empty line after table
    return table

def format_daily_price_table(data):
    """Format daily price data into a proper markdown table."""
    if not data or len(data) < 10:
        return ["*æ¯æ—¥è‚¡åƒ¹è³‡è¨Šè¼‰å…¥ä¸­...*\n"]
    
    table = []
    table.append("| æ—¥æœŸ | é–‹ç›¤åƒ¹ | æœ€é«˜åƒ¹ | æœ€ä½åƒ¹ | æ”¶ç›¤åƒ¹ | æˆäº¤é‡ |")
    table.append("|------|--------|--------|--------|--------|--------|")
    
    # Group data: date followed by 5 price/volume values
    i = 0
    row_count = 0
    while i < len(data) and row_count < 30:
        if re.match(r'202[45]/\d{2}/\d{2}', data[i]):
            # This is a date, collect next 5 values
            row = [data[i]]  # Start with date
            
            # Look for the next 5 numeric values
            values_collected = 0
            j = i + 1
            while j < len(data) and values_collected < 5:
                item = data[j].strip()
                if item and (item.replace('.', '').replace(',', '').isdigit() or 
                           ('.' in item and item.replace('.', '').replace(',', '').isdigit())):
                    row.append(item)
                    values_collected += 1
                j += 1
            
            # Fill missing values with '-'
            while len(row) < 6:
                row.append('-')
            
            # Add table row
            table.append(f"| {' | '.join(row)} |")
            row_count += 1
            i = j
        else:
            i += 1
    
    table.append("")
    return table

def format_ownership_table(data):
    """Format stock ownership distribution data into a proper markdown table."""
    if not data or len(data) < 10:
        return ["*è‚¡æ¬Šåˆ†æ•£è¡¨è¼‰å…¥ä¸­...*\n"]
    
    table = []
    table.append("| æ—¥æœŸ | 100å¼µä»¥ä¸‹æŒè‚¡æ¯”ä¾‹ | 100-1000å¼µæŒè‚¡æ¯”ä¾‹ | 1000å¼µä»¥ä¸ŠæŒè‚¡æ¯”ä¾‹ | ç¸½è‚¡æ±äººæ•¸ |")
    table.append("|------|-------------------|--------------------|--------------------|----------|")
    
    # Group data: date followed by 3 percentages and 1 number
    i = 0
    row_count = 0
    while i < len(data) and row_count < 25:
        if re.match(r'202[45]/\d{2}/\d{2}', data[i]):
            # This is a date, collect next 4 values
            row = [data[i]]
            
            # Look for 3 percentages and 1 number
            values_collected = 0
            j = i + 1
            while j < len(data) and values_collected < 4:
                item = data[j].strip()
                if item and ('%' in item or (item.replace(',', '').replace('.', '').isdigit() and len(item) > 3)):
                    row.append(item)
                    values_collected += 1
                j += 1
            
            # Fill missing values with '-'
            while len(row) < 5:
                row.append('-')
            
            # Add table row
            table.append(f"| {' | '.join(row)} |")
            row_count += 1
            i = j
        else:
            i += 1
    
    table.append("")
    return table

def scrape_poorstock(stock_id, base_dir="."):
    """
    Scrape stock data from poorstock.com for a specific stock ID.
    """
    base_path = Path(base_dir)
    poorstock_dir = base_path / "poorstock"
    poorstock_dir.mkdir(exist_ok=True)
    
    csv_file = base_path / "StockID_TWSE_TPEX.csv"
    results_file = poorstock_dir / "download_results.csv"
    
    try:
        # Load stock data to get company name
        if not csv_file.exists():
            print(f"âŒ Stock CSV file not found: {csv_file}")
            return False
        
        stock_df = pd.read_csv(csv_file)
        stock_row = stock_df[stock_df['ä»£è™Ÿ'] == stock_id]
        
        if stock_row.empty:
            print(f"âŒ Stock ID {stock_id} not found in CSV")
            return False
        
        stock_name = stock_row.iloc[0]['åç¨±']
        filename = f"poorstock_{stock_id}_{stock_name}.md"
        
        print(f"ğŸ“ˆ Processing stock: {stock_id} ({stock_name})")
        
        # Scrape the webpage with proper encoding handling
        url = f"https://poorstock.com/stock/{stock_id}"
        print(f"ğŸŒ Fetching: {url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Accept-Encoding': 'identity'
        }
        
        session = requests.Session()
        session.headers.update(headers)
        
        response = session.get(url, timeout=15)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        print(f"ğŸ“Š Page loaded successfully")
        
        # Get full text content
        full_text = soup.get_text()
        print(f"ğŸ“Š Total content length: {len(full_text)} characters")
        
        # Verify content quality
        has_chinese = any(ord(c) > 0x4e00 and ord(c) < 0x9fff for c in full_text[:200])
        print(f"âœ… Contains Chinese characters: {has_chinese}")
        
        if not has_chinese:
            print("âŒ Content appears to be unreadable")
            return False
        
        # Build content
        content_parts = []
        
        # Add title
        title = soup.find('title')
        if title:
            title_text = title.get_text().strip()
            content_parts.append(f"# {title_text}\n")
        
        # Extract data date information
        data_date_info = ""
        date_patterns = [
            r'è³‡æ–™æ—¥æœŸ[ï¼š:\s]*2025/\d{2}/\d{2}[^ã€‚]*?æ›´æ–°[ã€‚]?',
            r'2025/\d{2}/\d{2}[^ã€‚]*?æ¯å¤©\d{2}:\d{2}å¾Œæ›´æ–°',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, full_text, re.DOTALL)
            if match:
                data_date_info = re.sub(r'<[^>]+>', '', match.group(0).strip())
                data_date_info = re.sub(r'\s+', ' ', data_date_info)
                print(f"ğŸ“… Found date info: {data_date_info}")
                break
        
        if not data_date_info:
            date_part = re.search(r'2025/\d{2}/\d{2}', full_text)
            update_part = re.search(r'æ¯å¤©\d{2}:\d{2}å¾Œæ›´æ–°', full_text)
            if date_part and update_part:
                data_date_info = f"è³‡æ–™æ—¥æœŸï¼š{date_part.group(0)}ï¼Œ{update_part.group(0)}ã€‚"
        
        # Process content sections
        lines = [line.strip() for line in full_text.split('\n') if line.strip()]
        
        daily_price_data = []
        ownership_data = []
        current_price_data = []
        sections_added = set()
        current_section = ""
        
        # Extract data systematically
        for i, line in enumerate(lines):
            # Section detection
            if any(keyword in line for keyword in ["æ¯æ—¥è‚¡åƒ¹è³‡è¨Š", "æœ¬æ—¥è‚¡åƒ¹è³‡è¨Š", "Kç·šåœ–èˆ‡è‚¡åƒ¹æœ¬æ—¥äº¤æ˜“è³‡è¨Š"]):
                if "daily_price_section" not in sections_added:
                    content_parts.append("\n## æ¯æ—¥è‚¡åƒ¹è³‡è¨Š\n")
                    if data_date_info:
                        content_parts.append(f"**{data_date_info}**\n")
                    sections_added.add("daily_price_section")
                current_section = "daily_price"
                
            elif "AIçš„Kç·šåœ–åˆ†æ" in line or "è‚¡åƒ¹èµ°å‹¢åˆ†æ" in line:
                # Process tables before AI section
                if daily_price_data or current_price_data:
                    if current_price_data:
                        content_parts.extend(format_current_price_table(current_price_data, daily_price_data))
                        current_price_data = []
                    if daily_price_data:
                        content_parts.extend(format_daily_price_table(daily_price_data))
                        daily_price_data = []
                
                if "ai_analysis" not in sections_added:
                    content_parts.append("\n## AIè‚¡åƒ¹èµ°å‹¢åˆ†æèˆ‡æ“ä½œå»ºè­°\n")
                    sections_added.add("ai_analysis")
                current_section = "ai_analysis"
                
            elif "æ¯é€±è‚¡æ¬Šåˆ†æ•£è¡¨" in line:
                if "weekly_ownership" not in sections_added:
                    content_parts.append("\n### æ¯é€±è‚¡æ¬Šåˆ†æ•£è¡¨åˆ†ç´šè³‡æ–™\n")
                    sections_added.add("weekly_ownership")
                current_section = "ownership"
                
            elif "è©•è«–è¨è«–å€" in line:
                current_section = "skip"
                
            # Data collection
            elif current_section == "daily_price":
                # More aggressive current price detection
                if any(keyword in line for keyword in ['é–‹', 'é«˜', 'ä½', 'æ”¶']):
                    current_price_data.append(line)
                    # Look ahead for values in the next few lines
                    for j in range(i+1, min(i+8, len(lines))):
                        next_line = lines[j].strip()
                        if next_line and (next_line.replace('.', '').replace(',', '').isdigit() or 
                                        ('.' in next_line and next_line.replace('.', '').replace(',', '').isdigit())) and len(next_line) < 10:
                            current_price_data.append(next_line)
                            if len(current_price_data) >= 8:  # Headers + 4 values should be enough
                                break
                elif re.match(r'202[45]/\d{2}/\d{2}', line):
                    daily_price_data.append(line)
                elif line.replace('.', '').replace(',', '').isdigit() and '.' in line and len(line) < 10:
                    # Check if we're still in current price context (near headers)
                    recent_headers = any('é–‹' in l or 'é«˜' in l or 'ä½' in l or 'æ”¶' in l 
                                       for l in lines[max(0, i-5):i])
                    if recent_headers and len(current_price_data) < 12:
                        current_price_data.append(line)
                    else:
                        daily_price_data.append(line)
                        
            elif current_section == "ai_analysis" and len(line) > 20:
                if any(keyword in line for keyword in ["ä¸€ã€", "äºŒã€", "ä¸‰ã€", "å››ã€"]):
                    content_parts.append(f"\n### {line}\n")
                elif "å…ƒ" in line and any(keyword in line for keyword in ["æ”¯æ’", "å£“åŠ›", "ç›®æ¨™"]):
                    content_parts.append(f"\n**{line}**\n")
                elif len(line) > 30:
                    content_parts.append(f"{line}\n")
                    
            elif current_section == "ownership":
                if re.match(r'202[45]/\d{2}/\d{2}', line):
                    ownership_data.append(line)
                elif re.search(r'\d+\.\d+%', line) or (line.replace(',', '').isdigit() and len(line) > 3):
                    ownership_data.append(line)
        
        # Process remaining tables
        if daily_price_data or current_price_data:
            if current_price_data:
                content_parts.extend(format_current_price_table(current_price_data, daily_price_data))
            if daily_price_data:
                content_parts.extend(format_daily_price_table(daily_price_data))
        if ownership_data:
            content_parts.extend(format_ownership_table(ownership_data))
        
        # Add metadata
        content_parts.extend([
            f"\n---\n",
            f"**è‚¡ç¥¨ä»£è™Ÿ:** {stock_id}",
            f"**å…¬å¸åç¨±:** {stock_name}",
            f"**è³‡æ–™ä¾†æº:** {url}",
            f"**æŠ“å–æ™‚é–“:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        ])
        
        # Save content
        content = "\n".join(content_parts)
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        filepath = poorstock_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… Content saved to: {filepath}")
        print(f"ğŸ“„ Final content length: {len(content)} characters")
        
        # Update results CSV
        process_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        file_mod_time = datetime.fromtimestamp(filepath.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
        
        if results_file.exists():
            results_df = pd.read_csv(results_file)
        else:
            results_df = pd.DataFrame(columns=['filename', 'last_update_time', 'success', 'process_time'])
        
        mask = results_df['filename'] == filename
        if mask.any():
            results_df.loc[mask, 'last_update_time'] = file_mod_time
            results_df.loc[mask, 'success'] = True
            results_df.loc[mask, 'process_time'] = process_time
        else:
            new_record = pd.DataFrame([{
                'filename': filename,
                'last_update_time': file_mod_time,
                'success': True,
                'process_time': process_time
            }])
            results_df = pd.concat([results_df, new_record], ignore_index=True)
        
        results_df.to_csv(results_file, index=False)
        print(f"ğŸ“Š Results updated: {results_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def batch_process_all(base_dir="."):
    """Process all stocks from the CSV file."""
    base_path = Path(base_dir)
    csv_file = base_path / "StockID_TWSE_TPEX.csv"
    
    if not csv_file.exists():
        print(f"âŒ Stock CSV file not found: {csv_file}")
        return
    
    stock_df = pd.read_csv(csv_file)
    total_stocks = len(stock_df)
    
    print(f"ğŸš€ Starting batch processing of {total_stocks} stocks...")
    
    successful = 0
    failed = 0
    
    for i, (_, row) in enumerate(stock_df.iterrows(), 1):
        stock_id = row['ä»£è™Ÿ']
        stock_name = row['åç¨±']
        
        print(f"\n[{i}/{total_stocks}] Processing {stock_id} ({stock_name})")
        
        if scrape_poorstock(stock_id, base_dir):
            successful += 1
        else:
            failed += 1
        
        # Delay between requests
        import time
        time.sleep(2)
    
    print(f"\nğŸ‰ Batch processing complete!")
    print(f"âœ… Successful: {successful}")
    print(f"âŒ Failed: {failed}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage:")
        print("  python poorstock.py <stock_id>     # Process single stock")
        print("  python poorstock.py --all          # Process all stocks")
        print("  python poorstock.py 2412           # Process stock 2412 (ä¸­è¯é›»)")
        sys.exit(1)
    
    if sys.argv[1] == "--all":
        batch_process_all()
    else:
        try:
            stock_id = int(sys.argv[1])
            success = scrape_poorstock(stock_id)
            if success:
                print(f"\nğŸ‰ Successfully processed stock {stock_id}")
            else:
                print(f"\nğŸ˜ Failed to process stock {stock_id}")
                sys.exit(1)
        except ValueError:
            print("âŒ Invalid stock ID. Please provide a valid integer.")
            sys.exit(1)